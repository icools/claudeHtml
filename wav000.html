<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <title>廣播聲音效果視覺化</title>
    <style>
        body, html {
            margin: 0;
            padding: 0;
            overflow: hidden;
            background-color: #000;
        }
        #fileInput {
            position: absolute;
            top: 20px;
            left: 20px;
            z-index: 10;
            color: #fff;
        }
    </style>
</head>
<body>
    <input type="file" id="fileInput" accept="audio/*">

    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <script>
        let scene, camera, renderer, analyser, audioContext, source;
        let uniforms, mesh;

        function init() {
            // 初始化場景、相機和渲染器
            scene = new THREE.Scene();
            camera = new THREE.PerspectiveCamera(70, window.innerWidth / window.innerHeight, 1, 1000);
            camera.position.z = 50;

            renderer = new THREE.WebGLRenderer();
            renderer.setSize(window.innerWidth, window.innerHeight);
            document.body.appendChild(renderer.domElement);

            // 創建着色器材質
            uniforms = {
                time: { value: 1.0 },
                amplitude: { value: 0.0 },
                dataArray: { value: null }
            };

            const shaderMaterial = new THREE.ShaderMaterial({
                uniforms: uniforms,
                vertexShader: `
                    uniform float amplitude;
                    attribute float displacement;
                    varying vec3 vNormal;
                    void main() {
                        vNormal = normal;
                        vec3 newPosition = position + normal * vec3(displacement * amplitude);
                        gl_Position = projectionMatrix * modelViewMatrix * vec4(newPosition, 1.0);
                    }
                `,
                fragmentShader: `
                    varying vec3 vNormal;
                    void main() {
                        gl_FragColor = vec4(abs(vNormal), 1.0);
                    }
                `,
                wireframe: true
            });

            // 創建球體幾何體
            const geometry = new THREE.SphereGeometry(20, 128, 128);

            // 添加位移屬性
            const displacement = new Float32Array(geometry.attributes.position.count);
            for (let i = 0; i < displacement.length; i++) {
                displacement[i] = Math.random() * 0.5;
            }
            geometry.setAttribute('displacement', new THREE.BufferAttribute(displacement, 1));

            // 創建網格並添加到場景
            mesh = new THREE.Mesh(geometry, shaderMaterial);
            scene.add(mesh);

            // 音頻設置
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            analyser = audioContext.createAnalyser();
            analyser.fftSize = 256;

            uniforms.dataArray.value = new Uint8Array(analyser.frequencyBinCount);

            // 窗口調整
            window.addEventListener('resize', onWindowResize, false);

            animate();
        }

        function animate() {
            requestAnimationFrame(animate);

            if (uniforms.dataArray.value) {
                analyser.getByteFrequencyData(uniforms.dataArray.value);

                // 計算平均值作為振幅
                let sum = 0;
                for (let i = 0; i < uniforms.dataArray.value.length; i++) {
                    sum += uniforms.dataArray.value[i];
                }
                uniforms.amplitude.value = sum / uniforms.dataArray.value.length / 256;
            }

            uniforms.time.value += 0.05;

            mesh.rotation.y += 0.005;

            renderer.render(scene, camera);
        }

        function onWindowResize() {
            camera.aspect = window.innerWidth / window.innerHeight;
            camera.updateProjectionMatrix();

            renderer.setSize(window.innerWidth, window.innerHeight);
        }

        // 處理文件上傳
        document.getElementById('fileInput').addEventListener('change', function(event) {
            const file = event.target.files[0];
            if (file) {
                const reader = new FileReader();

                reader.onload = function(e) {
                    audioContext.decodeAudioData(e.target.result, function(buffer) {
                        if (source) {
                            source.stop();
                        }
                        source = audioContext.createBufferSource();
                        source.buffer = buffer;
                        source.connect(analyser);
                        analyser.connect(audioContext.destination);
                        source.start(0);
                    });
                };

                reader.readAsArrayBuffer(file);
            }
        });

        init();
    </script>
</body>
</html>